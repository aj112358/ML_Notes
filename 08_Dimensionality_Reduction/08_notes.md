# Ch 7 - Ensemble Learning and Random Forests

## Chapter Objectives

1. Learn various popular ensemble methods
    1. Bagging
    2. Boosting
    3. Stacking
2. Learn why ensemble methods work
    1. Law of large numbers
3. Learn about bagging and pasting
    1. Compare and contrast
    2. Out-of-bag evaluation
4. Learn about random patches and random subspaces
5. Learn about random forests
    1. Implementation in SKL
6. Learn about boosting
    1. AdaBoost
    2. Gradient Boost
7. Learn about stacking

## 7.1 - Voting Classifiers


## 7.2 - Bagging and Pasting


### 7.2.1 - Implementing Bagging and Pasting in SKL

### 7.2.2 - Out-of-Bag Evaluation


## 7.3 - Random Patches and Random Subspaces


## 7.4 - Random Forests

### 7.4.1 - Extra-Trees


### 7.4.2 - Feature Importance

## 7.5 - Boosting


### 7.5.1 - AdaBoost


### 7.5.2 - Gradient Boosting

### 7.5.3 - XGBoost (A Quick Note)


## 7.6 - Stacking
















## - Concluding Remarks


[anomaly_detection]: https://github.com/aj112358/ML_Notes/blob/main/01_The_Machine_Learning_Landscape/01_images/anomaly_detection.png "illustration of anomaly detection"
