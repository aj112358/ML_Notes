# Ch 4 - Training Models

## Chapter Objectives

1. Apply the ML process through a classification problem
2. Learn various performance measures for a classification problem
    1. Cross-Validation
    2. Confusion matrix
    3. Precision and recall
3. Learn about the trade-off between precision and recall
4. Implement a multiclass classifier
5. Learn about multilabel classification
6. Learn about multioutput classification

In the last chapter, we went balls-to-the-wall on a regression task and went through the entire ML process in great detail. In this chapter, we do the same for a classification task but focus more on various performance measures, and less on outlining the ML process. As it turns out, analyzing the performance of a classification task is very different from a regression task, so we will invest much time learning about this in detail. 


## 4.0 - 



## 4.1 - Linear Regression



### 4.1.1 - The Normal Equation
### 4.1.2 - Computational Complexity

## 4.2 - Gradient Descent

### 4.2.1 - Batch Gradient Descent
### 4.2.2 - Stochastic Gradient Descent
### 4.2.3 - Mini-Batch Gradient Descent





## 4.3 - Polynomial Regression


## 4.4 - Learning Curves

## 4.5 - Regularized Linear Models

### 4.5.1 - Ridge Regression
### 4.5.2 - Lasso Regression
### 4.5.3 - Elastic Net
### 4.5.4 - Early Stopping


## 4.6 - Logistic Regression
### 4.6.1 - Estimating Probabilities
### 4.6.2 - Training and Cost Function
### 4.6.3 - Decision Boundaries
### 4.6.4 - Softmax Regression

## - Concluding Remarks

In this chapter we have learned the many aspects of a classification task.





[anomaly_detection]: https://github.com/aj112358/ML_Notes/blob/main/01_The_Machine_Learning_Landscape/01_images/anomaly_detection.png "illustration of anomaly detection"
