# Ch 5 - Support Vector Machines

## Chapter Objectives

1. Formalize the linear regression ML algorithm
    1. Batch gradient descent

Thus far in our learning endeavors, we have learned about the overall ML process, created a full regression ML model, and created a classification ML model, all the while learning about various SKL functions and ways of thinking when implementing ML theory.

Now, we will invest some time in learning about the mathematics behind some of the ML algorithms we have been using. 


## 5.1 - Linear SVM Classification
### 5.1.1 - Soft Margin Classification

## 5.2 - Non-Linear SVM Classification
### 5.2.1 - Polynomial Kernal
### 5.2.2 - Similarity Features
### 5.2.3 - Gaussian RBF Kernal
### 5.2.4 - Computational Complexity

## 5.3 - SVM Regression

## 5.4 - Under the Hood
### 5.4.1 - Decision Function & Predictions
### 5.4.2 - Training Objective
### 5.4.3 - Quadratic Programming
### 5.4.4 - The Dual Problem
### 5.4.5 - Kernalized SVMs
### 5.4.6 - Online SVMs

## - Concluding Remarks







[anomaly_detection]: https://github.com/aj112358/ML_Notes/blob/main/01_The_Machine_Learning_Landscape/01_images/anomaly_detection.png "illustration of anomaly detection"
